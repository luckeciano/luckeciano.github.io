@inproceedings{10.1145/3383313.3412209,
author = {Santana, Marlesson R. O. and Melo, Luckeciano C. and Camargo, Fernando H. F. and Brand\~{a}o, Bruno and Soares, Anderson and Oliveira, Renan M. and Caetano, Sandor},
title = {Contextual Meta-Bandit for Recommender Systems Selection},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412209},
doi = {10.1145/3383313.3412209},
abstract = {Recommendation systems operate in a highly stochastic and non-stationary environment. As the amount of user-specific information varies, the users’ interests themselves also change. This combination creates a dynamic setting where a single solution will rarely be optimal unless it can keep up with these transformations. One system may perform better than others depending on the situation at hand, thus making the choice of which system to deploy, even more difficult. We address these problems by using the Hierarchical Reinforcement Learning framework. Our proposed meta-bandit acts as a policy over options, where each option maps to a pre-trained, independent recommender system. This meta-bandit learns online and selects a recommender accordingly to the context, adjusting to the situation. We conducted experiments on real data and found that our approach manages to address the dynamics within the user’s changing interests. We also show that it outperforms any of the recommenders separately, as well as an ensemble of them. },
booktitle = {Fourteenth ACM Conference on Recommender Systems},
pages = {444–449},
numpages = {6},
keywords = {contextual bandits, reinforcement learning, options framework, hierarchical recommender systems},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}